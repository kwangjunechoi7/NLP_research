### ALBERT paper review

BERT는 모델의 크기에 따라 base 모델과 large모델로 나뉘었음. 모델의 크기는 layer의 수와 layer의 hidden unit의 수를 결정하는 hidden size에 의해 결정되었음. 일반적으로 language representation모델은 layer의 hidden size의 수가 클때 더 높은 성능을 보였음.

그런데 무조건 모델을 키운다고 좋진 않더라.. language representation 모델의 크기가 클때 발생하는 문제점으로는
1. Memory limitation
2. Training time
3. Model degradation

#### F**actorized embedding parameterization**

Token embedding size를 hidden layer embedding size 보다 작게 설정하여 parameter수를 줄임.
- 기존 BERT, XLNet, RoBert 등 모두 E = H로 모델 구성
- H는 contextualized representation이지만, E는 context independent representation이므로 E<H 이어도 모델의 성능이 떨어지지는 않을 것

albert는 language representation 모델도 너무 커지면 성능이 떨어지느 것을 실험적으로 보여줌. 모델이 클 경우 메모리량이 부족하여 out of memory가 발생할 수 있음.


- bert에서는 input token embedding size와 hidden layer embedding size가 같음.

- Input token embedding은 token의 정보를 담고 있는 vector를 생성함. 이에 반해, trnasformer의 각 layer 에서의 outputd은 해당 token과 주변 token들 간의 관계까지 반영한 contextualized representation이다. 따라서 담고 있는 정보량 자체가 다르기 때문에 E가 H보다 비교적 작아도 될 것이라고 생각할 수 있음.

    - 하지만 E와 H가 달라지면 첫번재 transformer layer 입력시 차원이 맞지 않음. 이는 layer를 한 개 더 추가하는 방식으로 해결가능 v가 vocabulary size라면 기존 bert는 v*h matrix를 활용하여 token embedding을 함.
    - albert에서는 실험을 통해 e의 크기 변화에 따른 성능을 ㅂ보여줌.


#### C**ross-layer parameter sharing**
- Parameter embedding parmameterization보다 cross-layer parameter sharing 적용 시 모델의 크기가 훨씬 많이 줄어듦을 확인할 수 있음.
- Cross-layer parameter sharing은 말그대로 transformer layer같 같은 parameter를 공유하며 사용하는 것.
- Self-attention layer만 공유했을 때는 성능이 크게 떨어지지 않았다는 것이 확인됨. 그러나 feed forward network는 공유 시 성능이 다소 떨어지는 것이 보임. 논문에서는 실험결과에 대한 해석은 하지 않음.


#### S**entence order prediction**
- 논문에서는 next sentence prediction보다 나은 학습 방식인 sentence order prediction을 제안함. Nsp는 두 번째 문장이 첫 번째 문장의 다음 문장인지를 맞추는데 학습 데이터 구성 시 두 번째 문장은 실제 문장 positive example 혹은 임의로 뽑은 문장 negative example이 됨. 하지만 임의로 뽑은 문장은 첫번째 문장과 완전히 다른 topic 의 문장일 확률이 높으므로 문장 간 연간관계를 학습한다기 보다는 두 문장이 같은 topic 에 대해 말하는지를 판단하는 topic prediction에 가까움.
- 이에 반해 sentence order prediction의 학습 데이터는 실제 연속인 두 문장 positive example과 두 문장의 순서를 앞뒤로 바꾼 것 negative example으로 구성되고 문장의 순서가 옳은 지 여부를 예측하는 방식으로 학습함.
- 이것을 통해 학습 시 두 문장의 연관관계를 보다 잘 학습할 것이라 기대.
