# ALBERT(A Lite BERT For Self-Supervised Learning of Language Representations)

#### how to reduce parameters in ALBERT
- Word embedding을 hidden layer의 크기로 바로 맵핑하는 것이 아니라, 더 작은 값인 embedding size로 설정해주어 이후의 parameters 수를 전체적으로 줄임.

- BERT에서 hidden_size이었던 것이 ALBERT에서는 embedding_size로 바뀜. (최종 embedding size는 아님)

- attention과 feed forward layer간 layer에서 parameter를 공유하게 하여 하나의 뎁스에서만 parameters이 학습되는 것을 막음.

- 결과적으로 BERT보다 parameters가 18 줄었음.

- 다음 문장 예측보다 문장의 일관성을 예측하는 문제가 학습하는데 더 좋았는데 이러한 문제를 위해 설계된 loss가 SOP(Sentence-Order Prediction)loss.


### 보충 정리
- 메모리도 덜 먹고 빠른 BERT를 위해 parameter reduction technique을 두 가지 보인 연구.
- inter-sentence coherence를 modeling하기 위한 self-supervised learning loss 사용.
- 결과적으로 BERT large 보다 적은 파라미터로 더 좋은 결과를 냄.
- 첫 번째 parameter reduction technique은 embedding parameter를 factorizing하는 것. 두 번째는 cross layer parameter sharing. 또한 SOP라는 self-supervised loss를 만들어서 젹용.  



##### Reference
https://github.com/google-research/ALBERT



##### tutorial
