## SQuAD 2.0 paper review
Abstract
- Extractive reading comprehension system 들은 본문 내에서 질문에 대한 정확한 답을 찾아주기도 하지만, 제대로 된 답이 본문에 없을 시 부정확한 추측을 리턴하기도 함.
- 현존하는 reading comprehension 데이터셋들은 주로 answerable 질문들에 초점을 맞추거나 쉽게 판별이 가능한 가짜 unanswerable 질문들을 자동으로 생성해왔음.
- 이러한 약점을 보완하기 위해 만든것이 SQuAD 2.0
- SQuAD 2.0의 특징은 다음과 같음
    - 기존 데이터셋 SQuAD 1.0에 새로운 5만개 이상(53,775)의 unanswerable questions을 병합
    - Unanswerable question은 온라인의 crowd worker들이 직접 생성(즉, 기계적으로 생성된 것이 아니라 진짜 인간이 생성했으므로 질이 높음)
    - Workers에 의해 생성된 unanswerable question은 응답 가능한 질문들과 유사하여 기계적으로 판별이 어려움.
    - 이제 task가 단순히 정답이 가능할 때만 리턴하는 것이 아니고, 주어진 본문에 정답이 없는 경우도 그 여부를 리턴하는 것으로 좀 더 어려워짐.


Introduction
- SQuAD 1.1 dataset을 이용해 인간의 정확도 수준을 상회하는 기계독해 시스템들이 만들어지기도 했으나, 그럼에도 불구하고 이러한 시스템들의 진짜 언어 이해도는 떨어져있다고 판단함.
- 그 이유는 SQuAD가 정확한 답이 주어진 context내에 존재할 것이라는 가정에서출발하기 때문. 그러므로 모델은 정답이 정말 본문에 존재하는지 여부를 따지기 보다는 가장 답에 근접해보이는 span을 찾는데 집중하게 됨.
- 응답 불가능한 질문들은 응답 가능 질문들과 동일한 본문(paragraphs)에서 제작됨.
- 질문 제작에는 온라인의 다수 crowd worker들을 사용함.



Desidereta
- 저자들은 새로운 데이터 셋 제작 프로젝트에 있어 일반적인 large size , diversity, low noise 외에도 unanswerable question에 대해 두 가지 요구사항 desidereta를 정의.
- 1 . Relevance 연관성 : 응답 불가능 질문들을 주어진 본문과 주제적으로 연관성이 있어야 함. 그렇지 않으면 단순한 휴리스틱으로도 응답 가능/불가능 여부를 판별할 수있음.
- Existence of plausible answers : 본문 내에 불가능 질문이 요구하는 정답과 타입이 일치하는 그럴듯한 정답 구절이 포함되어 있어야 함. 예를 들어 1992년에 설립된 회사는 무엇인가? 라는 질문이 있을 경우 본문내에 어떠한 회사명이 좋재해야함. 그렇지 않으면 type-matching 휴리스틱이 응답/가능 불가능 여부를 판별할 수 있기 때문.
￼


Existing datasets
- 저자들은 이러한 조건 criteria들을 염두에 두면서 현존하는 다른 기계독해 데이터셋을 조사함. 응답 불가능한 질문과 짝을 이루는 본문을 지칭하기 위해 negative example이라는 용어를 사용.

- 3.1 Extractive datasets
- Extractive reading comprehension 데이터셋들은 주어진 본문 내에서 시스템이 정답을 추출해 냄.
- Zero-shot Relation Extraction dataset(Levy et al., 2017)
    - Distance supervision으로 만들어 낸 negative example 이 포함되어 있음.
    - Levy et al.은 negative example의 65%에는 그럴듯한 수준의 질문이 포함되어 있지 않아서 쉽게 판별이 가능하다는 것을 파악.
- TriviaQA(Joshi et al., 2017)
    - 또 다른 distance supervision approach
    - 각 질문에 대한 context 본문을 웹이나 위비피디아에서 검색해 사용함. 이렇게 수집된 본문 중 일부는 정답이 포함되어있지 않아 negative eample 이 됨. 하지만 이것들은 최종 데이터셑에 포함되지 않음.
- Negative examples for SQuAD(Clark and Gardner, 2017)
    - 기존에 있던 질의를 TF-IDF 오버랩 기반으로 동일한 문서의 다른 단락에 매핑
- 일반적으로 distant supervision은 검색된 context에서 그럴듯한 대답의 존재를 보장하지 않으며 오히려 주어진 context에 정답의 의역이 포함될 수 있으므로 잡읆을 추가할 수도 있음.
- NewsQA(Trischler et al., 2017)
    - 데이터수집과정에서 crowdworker들이 기사 원문이 아닌 요약만 이용하여 질문을 작성하기 때문에 대답할 수 없는 질문이 발생할 수 있음.
    - 하지만 질문들 가운데 오직 0.5% 만이 응답 불가능이므로 이 전략을 확장하기는 어려움.
        - 이 9.5% 중에서도 일부는 응답불가능으로 잘못 표기된 것이었고 다른 일부 범위 바깥 out-of-scope으로 판정.
- Jia and Liang(2017)은 SQuAD 질문을 약간 수정하여 규칙 기반의 응답 불가능 질문을 생성하는 방법을 제안.
    - 개체(entity)와 숫자들을 유사한 단어로 대체하고 명사와 형용사를 WordNet의 반의어로 대체하는 단순한 방식
    - 저자들은 이렇게 생성된 질문들을 rule-based질문들이라고 지칭


- 3.2 Answer sentence selection datasets
- Answer sentence selection은 시스템이 질문에 답하는 문장을 그렇지 않은 문장보다 높게 순위를 매길 수 있는지 여부를 테스트 함.

- 3.3 multiple choice dataset
- MCTest 또는 RACE 같은 일부 데이터셋에서만 정답 없음 옵션을 갖는 복수정답 질문을 포함하고 있었음.
- 실제로는 복수 정답 옵션은 사용할 수 없는 경우가 많으므로 이러한 데이터 세트는 사용자 대면 시스템 user-facing system을 학습하는데 적합하지 않음.
- 복수정답 질문은 추출식 extractive 질문과는 상당히 다른 경향이 있으며, 빈칸 채우기, 해석 및 요약 중점둠.


BERT to implement SQuAD tast
- Input 질의와 단락을 하나의 packed sequence로 표현함. (A: question embedding, B : paragraph embedding). Fine-tuning 과정에서 학습되는 것은 시작 벡터인  S∈R^H와 끝 벡터인 E∈R^H뿐.
