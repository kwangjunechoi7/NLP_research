## note for BERT
- query : vector associated with that word.
- key :
- value :

+ creating query, key, value projection of each word in the input Sentence

+ These vectors are created by multiplying the embedding by three matrices that we trained during the training process.

+ Notice that these new vectors are smaller indimension than the embedding vector Their dimensionality is 64, while the embedding and encoeder input/output vectores have dimensionality of 512. They don't have to be smaller, this is an architecture chice to make the computation of multiheaded attention constant.
