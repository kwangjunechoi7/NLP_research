## note for BERT
- query : vector associated with that word.
- key :
- value :

+ creating query, key, value projection of each word in the input Sentence

+ These vectors are created by multiplying the embedding by three matrices that we trained during the training process.

+ Notice that these new vectors are smaller indimension than the embedding vector Their dimensionality is 64, while the embedding and encoeder input/output vectores have dimensionality of 512. They don't have to be smaller, this is an architecture chice to make the computation of multiheaded attention constant.

+ model output
  - 각각의 position에 대해서 H size의 embedding vector가 출력됨.

+ pre-training
  + transformer는 encoder-decoder 구조로 decoder에서 loss function을 계산해서 training함.
  + BERT는 encoder만 있음. MLM, two sentenec tasks 2가지 방식을 사용.
