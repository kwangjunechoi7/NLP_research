{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Reading Comprehension 기계독해는 주어진 지문과 질문을 이해하여 지문 내에서 답변을 찾아야 하는 자연어 처리 과제. 질의응답기술의 핵심. \n",
    "\n",
    "\n",
    "- 최근 기계 독해 태스크도 다른 자연어처리 태스크와 유사하게 BERT와 같이 사전 학습된 언어 모델을 이용하고 지문과 질문이 입력되었을 때 답변의 경계를 추가 학습하는 방법을 활용. \n",
    "\n",
    "\n",
    "- 관련 연구, 기계독해는 자연어처리 분야의 중요 태스크 중 하나로서 주어진 지문과 질문을 이해하여 답변해야하는 과제. 질문과 답변 데이터 셋을 학습하여 질문에 대한 답변을 지문내에서 찾는 것을 목표로 함. \n",
    "\n",
    "\n",
    "- 한국어를 사전학습한 모델로 Multilingual BERT와 ETRI의 KorBERT가 있음. Multilingual BERT는 구글에서 공개한 100가지 이상의 다국어 지원 모델. 각 나라의 언어로 작성된 Wikipedia 문서의 텍스트를 이용해 학습한 모델. KorQuAD에서 성능이 좋게 나왔음. KorBERT는 한국어 특성을 반영한 형태소 분석 기반의 언어 모델. 신문 기사 및 백과사전 47억개의 형태소 학습함. \n",
    "\n",
    "\n",
    "- 사전 학습한 BERT 모델에 KorQuAD를 위한 출력 층 layer를 추가하고 성능 향상을 위하여 기계독해 질의으답에 의미있는 자질을 추출하고, 문맥 정보를 encoding하는 RNN을 적용함. \n",
    "\n",
    "\n",
    "- RNN은 gate layer의 이전 히든 스테이트 연산을 제거하여 학습 속도가 빠른 SRU(Simple Recurrent Unit)을 적용. SRU는 계산량이 적기 때문에 다른 RNN type(GRU, LSTM)들보다 레이어 스택을 깊게 쌓을 수 있고 highway network를 포함하고 있어 스택을 깊게 쌓는 경우 좋은 성능을 보이는 장점이 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
